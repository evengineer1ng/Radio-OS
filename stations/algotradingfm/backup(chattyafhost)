#!/usr/bin/env python3
"""
AlgoTrading FM â€” steady context engine + realtime host + multi-voice panel + async audio buffer (single file)

You said: "refactor in from here" â€” so this is your script, preserved, with additions layered in:
- Producer stays the same (slow model) and still enqueues *raw* segments (post+comments+angle).
- Host no longer speaks directly from LLM output.
  Instead, host LLM converts each queued segment into a STRUCTURED "show packet":
    {
      host_intro, summary,
      perspectives[{sentiment, voice, line}],
      host_takeaway, callback
    }
- A TTS worker renders that packet into an AUDIO BUNDLE (voice,text list) asynchronously.
- The host loop plays AUDIO BUNDLES continuously (never blocks on producer or heavy TTS).
- If audio buffer is low, host fills with short riffs while producer/tts catch up.
"""
from matplotlib.figure import Figure
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

import os, time, json, re, tempfile, subprocess, queue, sqlite3, random, hashlib

from typing import Any, Dict, List, Optional, Tuple

import requests
import sounddevice as sd
import soundfile as sf
import numpy as np
import tkinter as tk
from tkinter import ttk
import threading
from pycaw.pycaw import AudioUtilities, ISimpleAudioVolume
import asyncio
from winsdk.windows.media.control import (
    GlobalSystemMediaTransportControlsSessionManager as MediaManager
)

# =======================
# CONFIG
# =======================
subtitle_q: "queue.Queue[Tuple[str, Any]]" = queue.Queue()
# =======================
# DJ Event Bus (NEW)
# =======================
dj_q: "queue.Queue[Tuple[str, Any]]" = queue.Queue()

# event names:
# "bg" -> fade to background
# "spotlight" -> fade to 100
# "fade" -> fade to specific % (payload: int)
# "hype" -> fade out -> next -> 100
# "skip" -> next track
# "callin_on" -> fade to ~52
# "callin_off" -> fade back to 65

SHOW_NAME = "AlgoTrading FM"
HOST_NAME = "Kai"

# Reddit
SUBREDDITS = ["algotrading", "quant", "trading", "datascience", "cryptocurrency"]
CANDIDATES_PER_SUB = 12
TOP_COMMENTS = 6
HEADERS = {"User-Agent": "AlgoTradingFM/1.0"}

# Ollama
OLLAMA_URL = "http://localhost:11434/api/generate"
CONTEXT_MODEL = "rnj-1:8b"   # big/slow background producer
HOST_MODEL = "rnj-1:8b"           # fast voice host

# Piper
PIPER_BIN = r"C:\Users\evana\Downloads\piper_windows_amd64\piper\piper.exe"
HYPERLIQUID_INFO_URL = "https://api.hyperliquid.xyz/info"
HL_USER_ADDRESS = os.environ.get("HL_USER_ADDRESS", "").strip()  # YOUR 0x...
HL_POLL_SEC = 8
HL_MILESTONES_USD = [100, 250, 500, 1000, 2000, 5000]  # customize

# Voices (multi-voice panel)
# NOTE: keep your original VOICE as the host voice for backwards compatibility.
VOICE = r"C:\Users\evana\Documents\algotradingfm\voices\en_US-lessac-high.onnx"
VOICE_MAP = {
    "host": VOICE,

    "optimist": r"C:\Users\evana\Documents\algotradingfm\voices\en_US-hfc_female-medium.onnx",
    "skeptic": r"C:\Users\evana\Documents\algotradingfm\voices\en_GB-alan-medium.onnx",
    "engineer": r"C:\Users\evana\Documents\algotradingfm\voices\en_GB-alba-medium.onnx",
    "macro": r"C:\Users\evana\Documents\algotradingfm\voices\en_GB-southern_english_female-low.onnx",
    "coach": r"C:\Users\evana\Documents\algotradingfm\voices\en_US-danny-low.onnx",
}
def normalize_tags(tags: List[str]) -> List[str]:
    out = []
    for t in tags or []:
        t = t.lower().strip()
        t = re.sub(r"[^a-z0-9_]", "", t)
        t = re.sub(r"_+", "_", t)

        if len(t) < 3:
            continue

        if t not in out:
            out.append(t)

    return out[:8]   # cap to avoid noise
TAG_CATALOG = normalize_tags([
    "execution", "slippage", "fees", "latency", "market_impact",
    "risk_management", "drawdown", "position_sizing", "leverage",
    "market_regime", "trend_following", "mean_reversion",
    "portfolio_construction", "overfitting", "walk_forward",
    "live_vs_backtest", "data_quality", "infra", "monitoring",
    "order_types", "funding", "liquidations", "correlation",
    "stops", "take_profit", "tail_risk"
])

# Station pacing
HOST_IDLE_RIFF_SEC = 45          # if queue empty (or audio buffer low), host riffs for this long
HOST_BETWEEN_SEGMENTS_SEC = 2
QUEUE_TARGET_DEPTH = 8           # keep this many ready-to-air segments buffered (DB)
QUEUE_MAX_DEPTH = 20
PRODUCER_TICK_SEC = 90           # how often producer tries to top up


# Audio buffering (NEW)
AUDIO_TARGET_DEPTH = 5           # keep this many ready-to-play audio bundles buffered
AUDIO_MAX_DEPTH = 10
AUDIO_TICK_SLEEP = 0.05          # small sleep in audio player loop

# Persistence
DB_PATH = "fm_station.sqlite"
MEMORY_PATH = "fm_memory.json"

STRATEGY_REF_PATH = "strategy_reference.txt"
COACH_REF_PATH = "coach_career_mode.txt"

# =======================
# NEW: Station Event Buses
# =======================
market_event_q: "queue.Queue[StationEvent]" = queue.Queue()
coach_event_q: "queue.Queue[StationEvent]" = queue.Queue()
ui_q: "queue.Queue[Tuple[str, Any]]" = queue.Queue()   # ("highlight_comment", idx), ("flash_chart", event), ...
from dataclasses import dataclass, field
import math
from collections import deque

@dataclass
class StationEvent:
    source: str
    type: str
    ts: int
    symbol: Optional[str] = None
    timeframe: Optional[str] = None
    severity: float = 0.0          # 0..1
    priority: float = 50.0         # 0..100
    payload: Dict[str, Any] = field(default_factory=dict)

def clamp01(x: float) -> float:
    return max(0.0, min(1.0, x))

def ewma(prev: float, x: float, alpha: float) -> float:
    return prev + alpha * (x - prev)

def tf_to_seconds(tf: str) -> int:
    # supports "1m","5m","15m","1h"
    if tf.endswith("m"):
        return int(tf[:-1]) * 60
    if tf.endswith("h"):
        return int(tf[:-1]) * 3600
    return 60
@dataclass
class Candle:
    ts: int
    o: float
    h: float
    l: float
    c: float
    v: float

class CandleBuffer:
    def __init__(self, maxlen: int = 500):
        self.buf = deque(maxlen=maxlen)

    def add(self, c: Candle):
        self.buf.append(c)

    def closes(self) -> List[float]:
        return [x.c for x in self.buf]

    def highs(self) -> List[float]:
        return [x.h for x in self.buf]

    def lows(self) -> List[float]:
        return [x.l for x in self.buf]

    def last(self) -> Optional[Candle]:
        return self.buf[-1] if self.buf else None
def is_image_url(url: str) -> bool:
    return url.lower().endswith((".png", ".jpg", ".jpeg", ".webp"))
def download_temp_image(url: str) -> Optional[str]:
    try:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        fd, path = tempfile.mkstemp(suffix=".png")
        with os.fdopen(fd, "wb") as f:
            f.write(r.content)
        return path
    except Exception:
        return None
def set_coach_reference(text: str) -> None:
    with open(COACH_REF_PATH, "w", encoding="utf-8") as f:
        f.write(text)

def get_coach_reference(max_chars: int = 6000) -> str:
    if not os.path.exists(COACH_REF_PATH):
        return ""
    with open(COACH_REF_PATH, "r", encoding="utf-8") as f:
        return f.read()[:max_chars]

def calc_rsi(closes: List[float], period: int = 14) -> Optional[float]:
    if len(closes) < period + 1:
        return None
    gains, losses = 0.0, 0.0
    for i in range(-period, 0):
        d = closes[i] - closes[i-1]
        if d >= 0:
            gains += d
        else:
            losses -= d
    if losses <= 1e-12:
        return 100.0
    rs = gains / losses
    return 100.0 - (100.0 / (1.0 + rs))
def portfolio_can_talk(mem: Dict[str, Any], *, reason: str, force: bool = False) -> bool:
    """
    Global cooldown so the show doesn't keep talking about YOUR portfolio.
    Allowed when:
      - force=True (major event)
      - cooldown has elapsed since last portfolio airtime
    """
    st = mem.setdefault("portfolio_speech", {})
    now = now_ts()

    cooldown_sec = int(st.get("cooldown_sec", 45 * 60))  # 45 minutes default
    last = int(st.get("last_spoken_ts", 0))

    if force or (now - last) >= cooldown_sec:
        st["last_spoken_ts"] = now
        st["last_reason"] = reason
        mem["portfolio_speech"] = st
        save_memory(mem)
        return True

    return False


def portfolio_mark_spoken(mem: Dict[str, Any], reason: str) -> None:
    st = mem.setdefault("portfolio_speech", {})
    st["last_spoken_ts"] = now_ts()
    st["last_reason"] = reason
    mem["portfolio_speech"] = st
    save_memory(mem)

def calc_atr(highs: List[float], lows: List[float], closes: List[float], period: int = 14) -> Optional[float]:
    if len(closes) < period + 1:
        return None
    trs = []
    for i in range(-period, 0):
        h, l = highs[i], lows[i]
        prev_c = closes[i-1]
        tr = max(h - l, abs(h - prev_c), abs(l - prev_c))
        trs.append(tr)
    return sum(trs) / len(trs)

def calc_slope(closes: List[float], lookback: int = 20) -> Optional[float]:
    # simple normalized slope via linear fit on index
    if len(closes) < lookback:
        return None
    y = closes[-lookback:]
    n = len(y)
    x = list(range(n))
    x_mean = (n - 1) / 2.0
    y_mean = sum(y) / n
    num = sum((x[i] - x_mean) * (y[i] - y_mean) for i in range(n))
    den = sum((x[i] - x_mean) ** 2 for i in range(n)) + 1e-12
    slope = num / den
    # normalize by price level
    return slope / (y_mean + 1e-12)

def calc_realized_vol(closes: List[float], lookback: int = 30) -> Optional[float]:
    if len(closes) < lookback + 1:
        return None
    rets = []
    for i in range(-lookback, 0):
        r = math.log((closes[i] + 1e-12) / (closes[i-1] + 1e-12))
        rets.append(r)
    mu = sum(rets) / len(rets)
    var = sum((r - mu) ** 2 for r in rets) / max(len(rets) - 1, 1)
    return math.sqrt(var)
BINANCE_KLINES = "https://api.binance.com/api/v3/klines"

def fetch_binance_klines(symbol: str, interval: str, limit: int = 200) -> List[Candle]:
    params = {"symbol": symbol, "interval": interval, "limit": limit}
    r = requests.get(BINANCE_KLINES, params=params, timeout=8)
    r.raise_for_status()
    out = []
    for k in r.json():
        # kline schema: [openTime, open, high, low, close, volume, closeTime, ...]
        out.append(Candle(
            ts=int(k[0])//1000,
            o=float(k[1]), h=float(k[2]), l=float(k[3]), c=float(k[4]),
            v=float(k[5])
        ))
    return out
def chart_watcher_worker(
    stop_event: threading.Event,
    mem: Dict[str, Any],
    symbols: List[str],
    timeframes: List[str],
    poll_sec: float = 10.0
) -> None:
    """
    Non-blocking watcher:
    - polls recent candles
    - computes indicators
    - emits StationEvent into market_event_q
    - updates mem["market_heat"] (0..1)
    - pushes ui_q ("chart_update") for the primary symbol/tf (for StationUI chart)
    """
    store: Dict[Tuple[str, str], CandleBuffer] = {}
    last_emit: Dict[Tuple[str, str, str], int] = {}  # (symbol,tf,event_type)->ts

    mem.setdefault("market_heat", 0.0)

    # UI primary chart selection (can be overridden at runtime via mem)
    mem.setdefault("ui_primary_symbol", symbols[0] if symbols else "BTCUSDT")
    mem.setdefault("ui_primary_tf", timeframes[0] if timeframes else "1m")

    def cooldown_ok(symbol: str, tf: str, et: str, cd_sec: int) -> bool:
        k = (symbol, tf, et)
        now = now_ts()
        last = last_emit.get(k, 0)
        if now - last >= cd_sec:
            last_emit[k] = now
            return True
        return False

    while not stop_event.is_set():
        try:
            heat_samples: List[float] = []

            primary_symbol = str(mem.get("ui_primary_symbol", "BTCUSDT"))
            primary_tf = str(mem.get("ui_primary_tf", "1m"))

            for sym in symbols:
                for tf in timeframes:
                    key = (sym, tf)
                    if key not in store:
                        store[key] = CandleBuffer(maxlen=600)

                    candles = fetch_binance_klines(sym, tf, limit=200)
                    buf = store[key]

                    # Replace buffer (robust)
                    buf.buf.clear()
                    for c in candles:
                        buf.add(c)

                    closes = buf.closes()
                    highs = buf.highs()
                    lows = buf.lows()

                    atr = calc_atr(highs, lows, closes, period=14)
                    rsi = calc_rsi(closes, period=14)
                    slope = calc_slope(closes, lookback=24)
                    rv = calc_realized_vol(closes, lookback=30)

                    last = buf.last()
                    if not last or atr is None or rsi is None or slope is None or rv is None:
                        continue

                    # ---- UI CHART FEED ----
                    if sym == primary_symbol and tf == primary_tf:
                        ui_q.put((
                            "chart_update",
                            {
                                "symbol": sym,
                                "tf": tf,
                                "candles": [
                                    {"ts": c.ts, "o": c.o, "h": c.h, "l": c.l, "c": c.c, "v": c.v}
                                    for c in list(buf.buf)[-240:]
                                ]
                            }
                        ))

                    # ---- heat proxy ----
                    atr_norm = atr / max(last.c, 1e-12)
                    vol_norm = rv * math.sqrt(tf_to_seconds(tf) / 60.0)
                    heat = clamp01((atr_norm * 12.0) + (vol_norm * 8.0))
                    heat_samples.append(heat)

                    # ---- events ----
                    # 1) volatility spike
                    if heat > 0.65 and cooldown_ok(sym, tf, "volatility_spike", cd_sec=45):
                        evt = StationEvent(
                            source="chart",
                            type="volatility_spike",
                            ts=now_ts(),
                            symbol=sym,
                            timeframe=tf,
                            severity=heat,
                            priority=85.0 + 15.0 * heat,
                            payload={
                                "magnitude": "high" if heat > 0.8 else "medium",
                                "context": "range expansion / vol regime pop",
                                "atr_norm": atr_norm,
                                "rv": rv,
                                "rsi": rsi,
                                "slope": slope,
                                "last_price": last.c,
                            }
                        )
                        market_event_q.put(evt)
                        ui_q.put(("flash_chart", {"symbol": sym, "timeframe": tf, "type": evt.type, "severity": evt.severity}))

                    # 2) trend shift
                    slope_long = calc_slope(closes, lookback=80)
                    if slope_long is not None:
                        if abs(slope - slope_long) > 0.0008 and cooldown_ok(sym, tf, "trend_shift", cd_sec=90):
                            direction = "bullish" if slope > 0 else "bearish"
                            strength = clamp01(abs(slope - slope_long) / 0.0020)
                            evt = StationEvent(
                                source="chart",
                                type="trend_shift",
                                ts=now_ts(),
                                symbol=sym,
                                timeframe=tf,
                                severity=strength,
                                priority=82.0 + 12.0 * strength,
                                payload={
                                    "direction": direction,
                                    "strength": strength,
                                    "rsi": rsi,
                                    "slope_short": slope,
                                    "slope_long": slope_long,
                                    "last_price": last.c,
                                }
                            )
                            market_event_q.put(evt)
                            ui_q.put(("flash_chart", {"symbol": sym, "timeframe": tf, "type": evt.type, "severity": evt.severity}))

                    # 3) breakout
                    if len(closes) >= 60 and cooldown_ok(sym, tf, "breakout", cd_sec=120):
                        rng_hi = max(highs[-50:-1])
                        rng_lo = min(lows[-50:-1])

                        if last.c > rng_hi * 1.0015:
                            sev = clamp01((last.c / rng_hi - 1.0) / 0.01)
                            evt = StationEvent(
                                source="chart",
                                type="breakout",
                                ts=now_ts(),
                                symbol=sym,
                                timeframe=tf,
                                severity=sev,
                                priority=86.0 + 10.0 * sev,
                                payload={"direction": "up", "range_hi": rng_hi, "range_lo": rng_lo, "last_price": last.c}
                            )
                            market_event_q.put(evt)
                            ui_q.put(("flash_chart", {"symbol": sym, "timeframe": tf, "type": evt.type, "severity": evt.severity}))

                        elif last.c < rng_lo * 0.9985:
                            sev = clamp01((1.0 - last.c / rng_lo) / 0.01)
                            evt = StationEvent(
                                source="chart",
                                type="breakout",
                                ts=now_ts(),
                                symbol=sym,
                                timeframe=tf,
                                severity=sev,
                                priority=86.0 + 10.0 * sev,
                                payload={"direction": "down", "range_hi": rng_hi, "range_lo": rng_lo, "last_price": last.c}
                            )
                            market_event_q.put(evt)
                            ui_q.put(("flash_chart", {"symbol": sym, "timeframe": tf, "type": evt.type, "severity": evt.severity}))

            # ---- global market_heat ----
            if heat_samples:
                target = sum(heat_samples) / len(heat_samples)
                mem["market_heat"] = ewma(float(mem.get("market_heat", 0.0)), target, alpha=0.12)
                save_memory(mem)

        except Exception:
            pass

        time.sleep(poll_sec)

def event_to_segment(evt: StationEvent, mem: Dict[str, Any]) -> Dict[str, Any]:
    # dynamic blending based on market_heat
    mh = float(mem.get("market_heat", 0.0))

    pri = float(evt.priority)
    if evt.source == "chart":
        pri *= (1.0 + 0.35 * mh)
    elif evt.source == "reddit":
        pri *= (1.0 - 0.55 * mh)

    pri = max(0.0, min(100.0, pri))

    title = ""
    body = ""

    if evt.source == "chart":
        title = f"{evt.symbol} {evt.timeframe}: {evt.type.replace('_',' ')}"
        body = json.dumps(evt.payload, ensure_ascii=False)

    elif evt.source == "coach":
        title = evt.payload.get("title", "Coach check-in")
        body = evt.payload.get("body", "")

    else:
        title = evt.type
        body = json.dumps(evt.payload, ensure_ascii=False)

    return {
        "id": sha1(f"evt|{evt.source}|{evt.type}|{evt.symbol}|{evt.timeframe}|{evt.ts}|{random.random()}"),
        "post_id": sha1(f"evtkey|{evt.source}|{evt.type}|{evt.symbol}|{evt.timeframe}|{evt.ts}"),

        # Keep subreddit for legacy UI; it can be evt.source, fine
        "subreddit": evt.source,

        # NEW: explicit origin
        "source": evt.source,          # "chart" | "coach" | "portfolio" | ...
        "event_type": evt.type,        # "trend_shift" | "milestone" | "breakout" | ...

        "title": title[:240],
        "body": clamp_text(body, 1400),
        "comments": [],

        "angle": evt.payload.get("angle", "React to what changed and why it matters right now."),
        "why": evt.payload.get("why", "Live market/portfolio/coaching state."),
        "key_points": evt.payload.get("key_points", ["what changed", "why now", "decision impact"]),
        "priority": pri,
        "host_hint": evt.payload.get("host_hint", "Alrightâ€”quick live update.")
    }
def career_weeks_elapsed(mem: Dict[str, Any]) -> float:
    c = mem.get("career", {}) or {}
    start_ts = int(c.get("start_ts", now_ts()))
    return (now_ts() - start_ts) / (7 * 24 * 3600)

def pick_coach_messages(mem: Dict[str, Any]) -> List[Dict[str, str]]:
    """
    Returns a small set of coach messages that are relevant "right now"
    based on weeks elapsed and hit flags.
    We keep this tiny so it doesn't bloat the prompt.
    """
    c = mem.get("career", {}) or {}
    hit = c.setdefault("coach_hit", {})  # { "year1": true, ... }
    weeks = career_weeks_elapsed(mem)

    msgs: List[Dict[str, str]] = []

    # Example: Year 1 completed -> trigger #05
    if weeks >= 52 and not hit.get("year1", False):
        msgs.append({
            "id": "05",
            "title": "Year 1 completed",
            "body": (
                "You finished a full season. Audit like a coach:\n"
                "- How many stop_loss / force_exit events happened and why?\n"
                "- Did shorts carry the team again?\n"
                "- Did any one tweak change behavior more than expected?\n"
                "If behavior is stable, scale attention. If unstable, freeze features and fix fundamentals."
            )
        })

    # Example: Month 1 -> trigger #02 (optional)
    if weeks >= 4 and not hit.get("month1", False):
        msgs.append({
            "id": "02",
            "title": "First Month: donâ€™t get cute",
            "body": (
                "Your only job right now is no unforced errors. "
                "Watch left-open count + worst open drawdowns. "
                "If one pair keeps becoming the anchor, bench it."
            )
        })

    # Keep at most 2 to avoid prompt spam
    return msgs[:2]

def coach_prompt_context(mem: Dict[str, Any]) -> str:
    """
    Small structured block injected into host_packet_system.
    This is NOT the full coach file; it's the 'active check-in' cues.
    """
    msgs = pick_coach_messages(mem)
    if not msgs:
        return ""

    lines = []
    for m in msgs:
        lines.append(f"[#{m['id']}] {m['title']}\n{m['body']}")
    return "\n\n".join(lines)

def event_router_worker(
    stop_event: threading.Event,
    conn: sqlite3.Connection,
    mem: Dict[str, Any]
) -> None:
    while not stop_event.is_set():
        try:
            evt = market_event_q.get(timeout=0.25)
            seg = event_to_segment(evt, mem)
            db_enqueue_segment(conn, seg)
            bump_tag_heat(mem, normalize_tags(["market_regime", evt.type, "volatility"]), boost=seg["priority"] * 0.35)
            save_memory(mem)
        except queue.Empty:
            pass
        except Exception:
            pass

        try:
            evt = coach_event_q.get_nowait()
            seg = event_to_segment(evt, mem)
            db_enqueue_segment(conn, seg)
            save_memory(mem)
        except queue.Empty:
            pass
        except Exception:
            pass
def init_career(mem: Dict[str, Any]) -> None:
    c = mem.setdefault("career", {})
    c.setdefault("start_ts", now_ts())
    c.setdefault("hit_milestones", [])
    c.setdefault("next_checkin_ts", now_ts() + 7*24*3600)
    # you can tune these
    c.setdefault("major_milestones", [100, 250, 500, 1000, 2000, 5000, 10000, 25000, 50000])
    c.setdefault("last_equity", None)
    c.setdefault("pace_mode", "rookie")  # rookie|all_star|mvp

def classify_pace(weeks: float, equity: float) -> str:
    # Simple targets. Replace with your real playbook curve later.
    # Rookie: 1.02^week; All-star: 1.04^week; MVP: 1.06^week
    # assumes start at 100 for pace comparisons (relative only).
    base = 100.0
    rookie = base * (1.02 ** max(weeks, 0))
    all_star = base * (1.04 ** max(weeks, 0))
    mvp = base * (1.06 ** max(weeks, 0))
    if equity >= mvp:
        return "mvp"
    if equity >= all_star:
        return "all_star"
    return "rookie"

def coach_worker(stop_event: threading.Event, mem: Dict[str, Any]) -> None:
    init_career(mem)

    while not stop_event.is_set():
        try:
            c = mem["career"]

            # -----------------------
            # Portfolio state
            # -----------------------
            hl = mem.get("hl_state", {}) or {}
            equity = hl.get("last_equity", None)

            if equity is None:
                time.sleep(2.0)
                continue

            equity = float(equity)

            start_ts = int(c.get("start_ts", now_ts()))
            weeks = (now_ts() - start_ts) / (7 * 24 * 3600)

            # -----------------------
            # Pace classification
            # -----------------------
            pace = classify_pace(weeks, equity)
            c["pace_mode"] = pace
            c["last_equity"] = equity

            # -----------------------
            # Persistent coach triggers (time-based)
            # -----------------------
            coach_hit = c.setdefault("coach_hit", {})

            def fire_coach_message(key: str, title: str, body: str, priority: float = 88.0):
                coach_hit[key] = True
                save_memory(mem)

                evt = StationEvent(
                    source="coach",
                    type="coach_message",
                    ts=now_ts(),
                    severity=0.6,
                    priority=priority,
                    payload={
                        "title": title,
                        "body": body,
                        "angle": "Deliver this as a grounded long-horizon coaching hit. Paraphrase naturally.",
                        "why": "Career mode milestone reached.",
                        "key_points": ["discipline", "process health", "long-term compounding"],
                        "host_hint": "Alrightâ€”coach stepping in for a quick reality check."
                    }
                )
                coach_event_q.put(evt)

            # ---- Month 1 ----
            if weeks >= 4 and not coach_hit.get("month1", False):
                fire_coach_message(
                    "month1",
                    "First month â€” donâ€™t get cute",
                    "Early phase is about avoiding unforced errors. "
                    "Watch left-open counts and worst drawdowns. "
                    "If one pair keeps dragging, bench it."
                )

            # ---- Year 1 ----
            if weeks >= 52 and not coach_hit.get("year1", False):
                fire_coach_message(
                    "year1",
                    "Year 1 completed â€” full season audit",
                    "You finished a full season. Review exits, drawdowns, and which side carried returns. "
                    "If behavior is stable, scale attention. If unstable, freeze features and fix fundamentals.",
                    priority=95.0
                )

            # ---- Year 3 (rough compounding phase) ----
            if weeks >= 156 and not coach_hit.get("year3", False):
                fire_coach_message(
                    "year3",
                    "Year 3 â€” scalpers vs champions balance",
                    "By now scalpers should be the engine and champions the turbo. "
                    "If champ exits are too rare, gating may be strict. "
                    "If too common, risk may be creeping."
                )

            # ---- Year 5 ----
            if weeks >= 260 and not coach_hit.get("year5", False):
                fire_coach_message(
                    "year5",
                    "Year 5 review â€” confirm real edge",
                    "Pull full stats: win rate, average profit, worst drawdown, recovery time. "
                    "If metrics resemble backtests, youâ€™ve built real-world alpha."
                )

            # -----------------------
            # Weekly check-in
            # -----------------------
            if now_ts() >= int(c.get("next_checkin_ts", 0)):
                c["next_checkin_ts"] = now_ts() + 7 * 24 * 3600
                save_memory(mem)

                evt = StationEvent(
                    source="coach",
                    type="weekly_checkin",
                    ts=now_ts(),
                    severity=0.4,
                    priority=76.0,
                    payload={
                        "title": "Coach check-in â€” weekly tape review",
                        "body": f"Equity roughly {equity:.2f}. Weeks elapsed {weeks:.1f}. Current pace track: {pace}.",
                        "angle": "Zoom out and reinforce discipline over short-term noise.",
                        "why": "Maintains long-horizon narrative consistency.",
                        "key_points": [
                            "process over outcome",
                            "bounded losses",
                            "consistency",
                            "pace vs compounding"
                        ],
                        "host_hint": "Letâ€™s zoom out for a minute."
                    }
                )
                coach_event_q.put(evt)

            # -----------------------
            # Equity milestones (career narrative version)
            # -----------------------
            hit = set(c.get("hit_milestones", []))

            for m in c.get("major_milestones", []):
                if equity >= m and m not in hit:
                    hit.add(m)
                    c["hit_milestones"] = sorted(hit)
                    save_memory(mem)

                    # ðŸ”¥ DJ hype interrupt
                    dj_q.put(("hype", m))

                    evt = StationEvent(
                        source="coach",
                        type="milestone",
                        ts=now_ts(),
                        severity=1.0,
                        priority=99.0,
                        payload={
                            "title": f"Coach milestone â€” ${m:.0f} crossed",
                            "body": f"Portfolio crossed ${m:.0f}. Current equity around {equity:.2f}. Pace track: {pace}.",
                            "angle": "Celebrate briefly but reinforce discipline and risk framework.",
                            "why": "Milestones anchor the long career narrative.",
                            "key_points": [
                                "what worked",
                                "what must not change",
                                "next risk checkpoint"
                            ],
                            "host_hint": "Big moment â€” quick coach hit."
                        }
                    )
                    coach_event_q.put(evt)

        except Exception:
            # Never let coach thread kill the station
            pass

        time.sleep(1.0)

def set_strategy_reference(text: str) -> None:
    with open(STRATEGY_REF_PATH, "w", encoding="utf-8") as f:
        f.write(text)

def get_strategy_reference(max_chars: int = 4000) -> str:
    if not os.path.exists(STRATEGY_REF_PATH):
        return ""
    with open(STRATEGY_REF_PATH, "r", encoding="utf-8") as f:
        return f.read()[:max_chars]

# =======================
# Utilities
# =======================
async def get_spotify_session():
    sessions = await MediaManager.request_async()
    return sessions.get_current_session()
async def spotify_play_pause():
    s = await get_spotify_session()
    if s:
        await s.try_toggle_play_pause_async()


async def spotify_next():
    s = await get_spotify_session()
    if s:
        await s.try_skip_next_async()


async def spotify_prev():
    s = await get_spotify_session()
    if s:
        await s.try_skip_previous_async()


async def spotify_current_track():
    s = await get_spotify_session()
    if not s:
        return None

    info = s.get_media_properties_async()
    props = await info
    return {
        "title": props.title,
        "artist": props.artist,
        "album": props.album_title
    }


def get_spotify_volume():
    sessions = AudioUtilities.GetAllSessions()
    for s in sessions:
        if s.Process and s.Process.name().lower() == "spotify.exe":
            return s._ctl.QueryInterface(ISimpleAudioVolume)
    return None


def fade_spotify(target_percent: int, duration: float = 1.5, steps: int = 40):
    vol = get_spotify_volume()
    if not vol:
        return

    current = vol.GetMasterVolume()  # 0.0 - 1.0
    target = max(0.0, min(1.0, target_percent / 100.0))

    delta = (target - current) / steps
    delay = duration / steps

    for _ in range(steps):
        current += delta
        vol.SetMasterVolume(current, None)
        time.sleep(delay)

    vol.SetMasterVolume(target, None)

def hl_info(payload: Dict[str, Any], timeout: int = 12) -> Any:
    r = requests.post(HYPERLIQUID_INFO_URL, json=payload, timeout=timeout)
    r.raise_for_status()
    return r.json()

def hl_get_portfolio(user: str) -> Any:
    return hl_info({"type": "portfolio", "user": user})

def hl_get_clearinghouse(user: str) -> Any:
    return hl_info({"type": "clearinghouseState", "user": user})

def hl_get_fills(user: str) -> Any:
    return hl_info({"type": "userFills", "user": user, "aggregateByTime": True})

def ensure_heat_store(mem: Dict[str, Any]) -> None:
    if "tag_heat" not in mem:
        mem["tag_heat"] = {}
def bump_tag_heat(
    mem: Dict[str, Any],
    tags: List[str],
    boost: float = 10.0,
    default_half_life: float = 48.0
) -> None:
    ensure_heat_store(mem)
    now = now_ts()

    for tag in tags:
        tag = tag.lower().strip()
        if not tag:
            continue

        if tag not in mem["tag_heat"]:
            mem["tag_heat"][tag] = {
                "heat": 0.0,
                "half_life_hours": default_half_life,
                "last_touched": now
            }

        mem["tag_heat"][tag]["heat"] += boost
        mem["tag_heat"][tag]["last_touched"] = now
def decay_tag_heat(mem: Dict[str, Any]) -> None:
    ensure_heat_store(mem)
    now = now_ts()

    for tag, data in list(mem["tag_heat"].items()):
        heat = float(data.get("heat", 0))
        half_life = float(data.get("half_life_hours", 48))
        last = int(data.get("last_touched", now))

        if heat <= 0:
            continue

        dt_hours = max((now - last) / 3600.0, 0)

        # exponential decay
        decayed = heat * (0.5 ** (dt_hours / max(half_life, 0.01)))

        # kill dead topics
        if decayed < 0.5:
            del mem["tag_heat"][tag]
            continue

        data["heat"] = decayed
def pick_hot_tags(mem, k=3, min_heat=5.0, cooldown_sec=12*60, explore_prob=0.35):
    ensure_heat_store(mem)
    decay_tag_heat(mem)

    now = now_ts()
    last_spoken = mem.setdefault("tag_last_spoken", {})

    # Build weighted pool from heat
    pool = []
    weights = []
    for tag, data in mem["tag_heat"].items():
        heat = float(data.get("heat", 0))
        if heat < min_heat:
            continue

        # hard cooldown
        last = int(last_spoken.get(tag, 0))
        if now - last < cooldown_sec:
            continue

        pool.append(tag)
        weights.append(heat)

    chosen = []

    # Exploration: if pool is thin, or randomly, pull from catalog excluding recent
    recent = mem.get("recent_riff_tags", [])[-12:]
    recent_set = set(recent)

    def add_explore_tag():
        candidates = [t for t in TAG_CATALOG if t not in recent_set]
        if not candidates:
            candidates = TAG_CATALOG[:]  # fallback
        t = random.choice(candidates)
        if t not in chosen:
            chosen.append(t)

    # If we have no pool, we must explore
    if not pool:
        while len(chosen) < k:
            add_explore_tag()
    else:
        # Sample WITHOUT replacement (manual)
        for _ in range(k):
            if pool and (random.random() > explore_prob or len(chosen) == 0):
                # weighted pick from remaining
                t = random.choices(pool, weights=weights, k=1)[0]
                idx = pool.index(t)
                pool.pop(idx); weights.pop(idx)
                if t not in chosen:
                    chosen.append(t)
            else:
                add_explore_tag()

    # update memory: last spoken + recent list (unique append)
    for t in chosen:
        last_spoken[t] = now
        if t in mem.get("recent_riff_tags", [])[-50:]:
            # donâ€™t spam duplicates in the tail
            continue
        mem.setdefault("recent_riff_tags", []).append(t)

    mem["recent_riff_tags"] = mem["recent_riff_tags"][-60:]
    save_memory(mem)
    return chosen


RIFF_SHAPES = [
    "connect_two",     # connect two tags as a single thought
    "myth_bust",       # debunk a misconception
    "failure_mode",    # one nasty failure mode + mitigation
    "tradeoff",        # a tradeoff framing
    "producer_tease",  # tease an upcoming segment style
]
# =======================
# Hyperliquid helpers (PATCHED)
# =======================

def hl_maybe_enqueue(
    conn: sqlite3.Connection,
    mem: Dict[str, Any],
    title: str,
    body: str,
    tags: List[str],
    priority: float = 92.0,
    event_type: str = "update",
):
    seg_obj = {
        "id": sha1("hl|" + title + "|" + str(now_ts()) + "|" + str(random.random())),
        "post_id": sha1("hlpost|" + title + "|" + str(now_ts())),

        "subreddit": "hyperliquid",

        # stream-aware origin
        "source": "portfolio",
        "event_type": event_type,

        "title": title[:240],
        "body": body[:1400],
        "comments": [],

        "angle": "Portfolio update: interpret what changed and why it matters.",
        "why": "Live state from the trading account.",
        "key_points": ["what changed", "risk/exposure", "decision impact"],
        "priority": float(priority),
        "host_hint": "Alrightâ€”quick check-in on the live blotter."
    }

    db_enqueue_segment(conn, seg_obj)
    bump_tag_heat(mem, normalize_tags(tags), boost=priority * 0.45)
    save_memory(mem)


# =======================
# Hyperliquid worker (PATCHED)
# =======================

def hyperliquid_worker(
    stop_event: threading.Event,
    conn: sqlite3.Connection,
    mem: Dict[str, Any]
) -> None:

    if not HL_USER_ADDRESS:
        return

    state = mem.setdefault("hl_state", {
        "last_fill_ts": 0,
        "last_positions_sig": "",
        "last_equity": None,
        "prev_equity": None,

        # unified throttles
        "last_emit_ts": 0,

        "hit_milestones": []
    })

    # -----------------------
    # pacing controls
    # -----------------------
    MIN_EMIT_GAP = 45 * 60        # 45 min between minor chatter
    MIN_EQUITY_DELTA = 0.005     # 0.5% = minor
    BIG_EQUITY_DELTA = 0.01      # 1% = major (break cooldown)

    POSITION_SIG_LIMIT = 6000

    while not stop_event.is_set():
        try:
            ch = hl_get_clearinghouse(HL_USER_ADDRESS)
            now = now_ts()

            positions = ch.get("assetPositions", []) or []

            # ==================================================
            # A) POSITION CHANGES â†’ major-ish event
            # ==================================================

            sig_src = json.dumps(positions, sort_keys=True)[:POSITION_SIG_LIMIT]
            pos_sig = sha1(sig_src)

            if pos_sig != state["last_positions_sig"]:
                state["last_positions_sig"] = pos_sig

                # allow through (still lightly gated)
                if now - state["last_emit_ts"] > MIN_EMIT_GAP / 2:
                    state["last_emit_ts"] = now

                    hl_maybe_enqueue(
                        conn,
                        mem,
                        title="Portfolio positioning shifted",
                        body="Active positions changed across the book.",
                        tags=["portfolio_update", "risk_management", "execution"],
                        priority=94.0,
                        event_type="positions_change"
                    )

            # ==================================================
            # B) EQUITY TRACKING (minor vs major)
            # ==================================================

            summary = ch.get("marginSummary") or ch.get("crossMarginSummary") or {}

            equity = None
            for k in ("accountValue", "equity"):
                try:
                    if summary.get(k) is not None:
                        equity = float(summary[k])
                        break
                except Exception:
                    pass

            if equity is not None:
                prev = state.get("prev_equity")
                state["prev_equity"] = equity
                state["last_equity"] = equity

                if prev is not None:
                    frac = abs(equity - prev) / max(prev, 1e-9)

                    # ----- BIG MOVE (break cooldown) -----
                    if frac >= BIG_EQUITY_DELTA:
                        state["last_emit_ts"] = now

                        hl_maybe_enqueue(
                            conn,
                            mem,
                            title="Live equity spike on Hyperliquid",
                            body=f"Equity shifted sharply from {prev:.2f} to {equity:.2f}",
                            tags=["portfolio_update", "risk_management"],
                            priority=97.0,
                            event_type="risk_spike"
                        )

                    # ----- MINOR MOVE (gated) -----
                    elif frac >= MIN_EQUITY_DELTA and now - state["last_emit_ts"] > MIN_EMIT_GAP:
                        state["last_emit_ts"] = now

                        hl_maybe_enqueue(
                            conn,
                            mem,
                            title="Live equity moved on Hyperliquid",
                            body=f"Equity moved from {prev:.2f} to {equity:.2f}",
                            tags=["portfolio_update"],
                            priority=92.0,
                            event_type="update"
                        )

            # ==================================================
            # C) FILLS â†’ minor chatter (hard gated)
            # ==================================================

            fills = hl_get_fills(HL_USER_ADDRESS) or []

            newest = state["last_fill_ts"]
            new_fills = []

            for f in fills:
                ts = int(f.get("time", 0) or f.get("timestamp", 0))
                if ts > newest:
                    newest = max(newest, ts)
                    new_fills.append(f)

            if new_fills:
                state["last_fill_ts"] = newest

                if now - state["last_emit_ts"] > MIN_EMIT_GAP:
                    state["last_emit_ts"] = now

                    hl_maybe_enqueue(
                        conn,
                        mem,
                        title=f"Recent executions ({len(new_fills)})",
                        body="Several new trades were executed across the book.",
                        tags=["execution", "portfolio_update"],
                        priority=96.0,
                        event_type="fills"
                    )

            # ==================================================
            # D) MILESTONES â†’ always major
            # ==================================================

            hit = set(state.get("hit_milestones", []))

            for m in HL_MILESTONES_USD:
                if equity is not None and equity >= m and m not in hit:
                    hit.add(m)

                    dj_q.put(("hype", m))

                    state["last_emit_ts"] = now

                    hl_maybe_enqueue(
                        conn,
                        mem,
                        title=f"Milestone crossed: ${m:.0f}",
                        body=f"Portfolio crossed ${m:.0f}. Current equity ~{equity:.2f}",
                        tags=["milestone", "portfolio_update"],
                        priority=98.0,
                        event_type="milestone"
                    )

            state["hit_milestones"] = sorted(hit)

            mem["hl_state"] = state
            save_memory(mem)

        except Exception:
            pass

        time.sleep(HL_POLL_SEC)

def ollama_vision_generate(prompt: str, model="llava", image_path=None):
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
    }

    if image_path:
        with open(image_path, "rb") as f:
            payload["images"] = [f.read().hex()]

    r = requests.post(OLLAMA_URL, json=payload, timeout=120)
    r.raise_for_status()
    return (r.json().get("response") or "").strip()


def next_riff_shape(mem: Dict[str, Any]) -> str:
    lru = mem.setdefault("riff_style_lru", [])
    # simple rotation
    if not lru:
        lru = RIFF_SHAPES[:]
        random.shuffle(lru)
    shape = lru.pop(0)
    mem["riff_style_lru"] = lru
    save_memory(mem)
    return shape

def heat_riff_prompt(mem: Dict[str, Any]) -> str:
    hot_tags = pick_hot_tags(mem, k=3)
    shape = next_riff_shape(mem)

    if not hot_tags:
        return evergreen_riff(mem)

    return f"""
You have a short gap on air. Talk naturally for about {HOST_IDLE_RIFF_SEC} seconds.

Tags to touch:
{hot_tags}

Riff shape: {shape}

Rules:
- Spoken conversational radio tone, mid-show
- No bullet points
- No disclaimers
- Donâ€™t repeat yesterdayâ€™s phrasing; new angle
""".strip()
def enqueue_cold_open(conn: sqlite3.Connection, mem: Dict[str, Any]) -> None:
    # Only seed if queue is empty
    if db_queue_depth(conn) > 0:
        return

    cold = [
        ("coldopen_1", "What breaks first when you go live",
         ["monitoring", "execution", "slippage", "fees"]),

        ("coldopen_2", "Backtest confidence versus reality",
         ["overfitting", "walk_forward", "data_quality", "live_vs_backtest"]),

        ("coldopen_3", "Risk systems that actually matter",
         ["drawdown", "position_sizing", "leverage", "stops"]),

        ("coldopen_4", "Why trading regimes quietly kill edges",
         ["market_regime", "trend_following", "mean_reversion", "correlation"]),
    ]

    # Pick ONE cold open (radio style)
    pid, title, tags = random.choice(cold)

    seg_obj = {
        "id": sha1(pid + "|" + str(now_ts()) + "|" + str(random.random())),
        "post_id": pid,

        "subreddit": "station",

        # NEW
        "source": "station",
        "event_type": "cold_open",

        "title": title,
        "body": title,
        "comments": [],

        "angle": "Open with practical insight and real-world experience.",
        "why": "Sets the tone with evergreen, high-signal trading lessons.",
        "key_points": ["real failure modes", "hidden tradeoffs", "what actually matters live"],
        "priority": 95.0,
        "host_hint": "Alrightâ€”quick thought before we jump into todayâ€™s threads."
    }


    db_enqueue_segment(conn, seg_obj)

    # Seed heat so riffs + producer pick up these themes naturally
    bump_tag_heat(mem, normalize_tags(tags), boost=40.0)

    save_memory(mem)


def clean(t: str) -> str:
    if not t:
        return ""

    # remove anything inside square brackets
    t = re.sub(r"\[.*?\]", "", t)

    # remove markdown emphasis
    t = re.sub(r"\*+", "", t)
    t = re.sub(r"_+", "", t)
    t = re.sub(r"~+", "", t)

    # remove markdown links [text](url) -> text
    t = re.sub(r"\[(.*?)\]\(.*?\)", r"\1", t)

    # remove the word "kai" (case-insensitive, whole word)
    t = re.sub(r"\bkai\b", "", t, flags=re.IGNORECASE)

    # remove : and ;
    t = t.replace(":", "").replace(";", "")

    # collapse whitespace
    t = t.replace("\r", " ")
    t = re.sub(r"\s+", " ", t.strip())
    t = re.sub(r"\b(sure|here's|here is|let's talk about|i will now)\b.*", "", t, flags=re.I)

    return t


def now_ts() -> int:
    return int(time.time())

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()

def clamp_text(s: str, n: int) -> str:
    s = s or ""
    s = s.strip()
    if len(s) <= n:
        return s
    return s[:n] + "â€¦"

def load_memory() -> Dict[str, Any]:
    if os.path.exists(MEMORY_PATH):
        try:
            with open(MEMORY_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return {
        "themes": [],
        "callbacks": [],
        "last_station_id": 0,
        "recent_riff_tags": [],
        "tag_heat": {},
        "tag_last_spoken": {},   # NEW: per-tag cooldown tracking
        "riff_style_lru": [],    # NEW: rotate riff â€œshapesâ€
    }


def save_memory(mem: Dict[str, Any]) -> None:
    with open(MEMORY_PATH, "w", encoding="utf-8") as f:
        json.dump(mem, f, indent=2)
class SpotifyDJ:
    DEFAULT_BG = 65

    def play_pause(self):
        asyncio.run(spotify_play_pause())

    def skip(self):
        asyncio.run(spotify_next())

    def fade_to_bg(self):
        fade_spotify(self.DEFAULT_BG, 1.5)

    def fade_out(self):
        fade_spotify(0, 1.2)

    def spotlight(self):
        fade_spotify(100, 1.5)

    def interrupt_with_hype(self):
        self.fade_out()
        asyncio.run(spotify_next())   # assuming hype track queued
        self.spotlight()

    def resume_bg(self):
        self.fade_to_bg()

    def current_track(self):
        return asyncio.run(spotify_current_track())
def dj_worker(stop_event: threading.Event) -> None:
    dj = SpotifyDJ()

    # ensure background level on boot
    try:
        dj.fade_to_bg()
    except Exception:
        pass

    while not stop_event.is_set():
        try:
            evt, payload = dj_q.get(timeout=0.25)
        except queue.Empty:
            continue

        try:
            if evt == "bg":
                dj.fade_to_bg()

            elif evt == "spotlight":
                dj.spotlight()

            elif evt == "fade":
                # payload: percent int
                fade_spotify(int(payload), duration=1.2)

            elif evt == "hype":
                dj.interrupt_with_hype()

            elif evt == "skip":
                dj.skip()

            elif evt == "callin_on":
                fade_spotify(52, duration=0.5)

            elif evt == "callin_off":
                dj.fade_to_bg()

        except Exception:
            # never kill the station for DJ issues
            pass

# =======================
# Audio (Piper + playback)
# =======================
def play_wav(path: str) -> None:
    # wait briefly for file to fully flush
    for _ in range(10):
        if os.path.exists(path) and os.path.getsize(path) > 44:  # WAV header ~44 bytes
            break
        time.sleep(0.02)

    try:
        data, sr = sf.read(path, dtype="float32")
    except Exception as e:
        print("âš ï¸ Audio read failed, skipping chunk:", e)
        return

    if data.ndim == 1:
        data = data.reshape(-1, 1)

    sd.play(data, sr)
    sd.wait()


def speak(text: str, voice_key="host"):
    text = clean(text)
    if not text:
        return

    voice = VOICE_MAP.get(voice_key, VOICE_MAP["host"])

    words = text.split()
    prefix = f"{voice_key.upper()}: "

    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        wav = f.name

    subprocess.run(
        [PIPER_BIN, "-m", voice, "-f", wav],
        input=text,
        text=True,
        encoding="utf-8",
        errors="ignore",
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )


    if not os.path.exists(wav) or os.path.getsize(wav) < 100:
        return

    # Load audio
    data, sr = sf.read(wav, dtype="float32")
    if data.ndim == 1:
        data = data.reshape(-1, 1)

    duration = len(data) / sr
    word_time = max(duration / max(len(words), 1), 0.08)

    # Start playback
    sd.play(data, sr)

    # Word-by-word subtitles
    for i in range(len(words)):
        before = " ".join(words[:i])
        current = words[i]
        after = " ".join(words[i+1:])

        display = f"{prefix}{before} {current} {after}".strip()

        subtitle_q.put(display)

        time.sleep(word_time)

    sd.wait()

    try:
        os.remove(wav)
    except:
        pass



def play_audio_bundle(bundle: List[Tuple[str, str]]) -> None:
    """
    bundle: list of (voice_key, text)
    """
    for voice_key, text in bundle:
        speak(text, voice_key)

# =======================
# Ollama
# =======================
class StationUI:
    def __init__(self):
        self.root = tk.Tk()
        self.root.title("AlgoTrading FM â€” Live")
        self.root.geometry("1400x820")
        self.root.configure(bg="#0e0e0e")

        # Top-level grid
        self.root.rowconfigure(0, weight=1)
        self.root.rowconfigure(1, weight=0)
        self.root.columnconfigure(0, weight=1)

        main = tk.Frame(self.root, bg="#0e0e0e")
        main.grid(row=0, column=0, sticky="nsew")
        main.rowconfigure(0, weight=1)
        main.columnconfigure(0, weight=2)  # left meta
        main.columnconfigure(1, weight=5)  # body
        main.columnconfigure(2, weight=4)  # comments+chart

        # LEFT: meta
        self.meta = tk.Text(main, height=10, wrap="word", bg="#121212", fg="#e8e8e8", font=("Segoe UI", 12))
        self.meta.grid(row=0, column=0, sticky="nsew", padx=8, pady=8)
        self.meta.config(state="disabled")

        # MIDDLE: body
        self.body = tk.Text(main, wrap="word", bg="#111111", fg="#e8e8e8", font=("Segoe UI", 12))
        self.body.grid(row=0, column=1, sticky="nsew", padx=8, pady=8)

        # RIGHT: comments (top) + chart (bottom)
        right = tk.Frame(main, bg="#0e0e0e")
        right.grid(row=0, column=2, sticky="nsew", padx=8, pady=8)
        right.rowconfigure(0, weight=3)
        right.rowconfigure(1, weight=2)
        right.columnconfigure(0, weight=1)

        self.comments = tk.Text(right, wrap="word", bg="#101010", fg="#e8e8e8", font=("Segoe UI", 11))
        self.comments.grid(row=0, column=0, sticky="nsew", pady=(0, 8))

        # chart area
        self.fig = Figure(figsize=(5, 3), dpi=100)
        self.ax = self.fig.add_subplot(111)
        self.ax.set_title("Live Chart")
        self.canvas = FigureCanvasTkAgg(self.fig, master=right)
        self.canvas.get_tk_widget().grid(row=1, column=0, sticky="nsew")

        # BOTTOM: subtitles
        bottom = tk.Frame(self.root, bg="#0e0e0e")
        bottom.grid(row=1, column=0, sticky="ew")
        bottom.columnconfigure(0, weight=1)

        self.sub_label = tk.Label(
            bottom,
            text="",
            font=("Segoe UI", 20),
            fg="#e8e8e8",
            bg="#0e0e0e",
            justify="center"
        )
        self.sub_label.grid(row=0, column=0, sticky="ew", padx=16, pady=10)

        def _resize_subtitles(event):
            # keep subtitles inside window width with padding
            new_wrap = min(max(event.width - 40, 300), 1200)
            self.sub_label.config(wraplength=new_wrap)

        bottom.bind("<Configure>", _resize_subtitles)

        # comment highlighting tag
        self.comments.tag_configure("hl", background="#2a2a2a", foreground="#ffffff")

        # state for chart
        self.chart_state = {"symbol": "BTCUSDT", "tf": "1m", "candles": []}
        self.flash_until = 0

        self.root.after(40, self._poll_queues)

    def set_segment_display(self, seg: Dict[str, Any]):
        # meta text
        meta = (
            f"SOURCE: {seg.get('subreddit','')}\n"
            f"TITLE: {seg.get('title','')}\n"
            f"POST_ID: {seg.get('post_id','')}\n"
        )
        self.meta.config(state="normal")
        self.meta.delete("1.0", "end")
        self.meta.insert("1.0", meta)
        self.meta.config(state="disabled")

        self.body.delete("1.0", "end")
        self.body.insert("1.0", seg.get("body",""))

        # comments
        self.comments.delete("1.0", "end")
        for i, c in enumerate(seg.get("comments", []) or []):
            self.comments.insert("end", f"[{i}] {c}\n\n")

    def highlight_comment(self, idx: int):
        self.comments.tag_remove("hl", "1.0", "end")
        # find line containing "[idx]"
        target = f"[{idx}]"
        start = self.comments.search(target, "1.0", stopindex="end")
        if start:
            end = f"{start} lineend"
            self.comments.tag_add("hl", start, end)
            self.comments.see(start)

    def flash_chart(self, severity: float):
        self.flash_until = now_ts() + int(1 + 2*severity)

    def update_chart(self, symbol: str, tf: str, candles: List[Candle]):
        self.chart_state = {"symbol": symbol, "tf": tf, "candles": candles}
        self.redraw_chart()

    def redraw_chart(self):
        st = self.chart_state
        candles = st.get("candles", [])
        if not candles:
            return
        closes = [c.c for c in candles[-120:]]
        xs = list(range(len(closes)))

        self.ax.clear()
        self.ax.plot(xs, closes)
        self.ax.set_title(f"{st['symbol']} {st['tf']} (close)")

        if now_ts() <= self.flash_until:
            self.ax.set_facecolor("#1c1c1c")  # subtle flash (kept minimal)
        else:
            self.ax.set_facecolor("#ffffff")

        self.canvas.draw_idle()

    def set_subtitle(self, text: str):
        self.sub_label.config(text=text)

    def _poll_queues(self):
        # subtitles
        try:
            while True:
                t = subtitle_q.get_nowait()
                self.set_subtitle(t)
        except queue.Empty:
            pass

        # UI cues
        try:
            while True:
                evt, payload = ui_q.get_nowait()

                if evt == "highlight_comment":
                    self.highlight_comment(int(payload))

                elif evt == "flash_chart":
                    self.flash_chart(float(payload.get("severity", 0.5)))

                elif evt == "chart_update":
                    # payload: {"symbol": str, "tf": str, "candles": [{ts,o,h,l,c,v}, ...]}
                    try:
                        candles = [Candle(**c) for c in payload.get("candles", [])]
                        if candles:
                            self.update_chart(payload.get("symbol", "BTCUSDT"), payload.get("tf", "1m"), candles)
                    except Exception:
                        pass

                elif evt == "set_segment_display":
                    # optional: allow workers to push full segment display
                    if isinstance(payload, dict):
                        self.set_segment_display(payload)

        except queue.Empty:
            pass

        # refresh chart flash decay
        if self.flash_until:
            self.redraw_chart()

        self.root.after(60, self._poll_queues)


def ollama(prompt: str, system: str, model: str, num_predict: int, temperature: float) -> str:
    payload = {
        "model": model,
        "prompt": prompt,
        "system": system,
        "stream": False,
        "options": {
            "temperature": temperature,
            "num_predict": num_predict,
        }
    }
    r = requests.post(OLLAMA_URL, json=payload, timeout=600)
    r.raise_for_status()
    j = r.json()
    return (j.get("response") or "").strip()

# =======================
# Reddit (public JSON)
# =======================

def fetch_sub_posts(sub: str, limit: int) -> List[Dict[str, Any]]:
    urls = [
        f"https://www.reddit.com/r/{sub}/new.json?limit={limit}",
        f"https://www.reddit.com/r/{sub}/hot.json?limit={limit}",
    ]
    items: List[Dict[str, Any]] = []
    for url in urls:
        try:
            data = requests.get(url, headers=HEADERS, timeout=15).json()
            for ch in data.get("data", {}).get("children", []):
                p = ch.get("data", {})
                if p.get("stickied"):
                    continue
                items.append(p)
        except Exception:
            continue
    return items

def fetch_comments(post_id: str, limit: int) -> List[str]:
    url = f"https://www.reddit.com/comments/{post_id}.json?limit={limit}"
    data = requests.get(url, headers=HEADERS, timeout=15).json()
    out: List[str] = []
    try:
        for c in data[1]["data"]["children"]:
            if c.get("kind") == "t1":
                body = c["data"].get("body", "")
                if body:
                    out.append(body[:450])
            if len(out) >= limit:
                break
    except Exception:
        pass
    return out

def candidate_score(p: Dict[str, Any]) -> float:
    score = 0.0
    score += min(int(p.get("score", 0)), 800) / 10.0
    score += min(int(p.get("num_comments", 0)), 300) / 5.0

    title = (p.get("title") or "").lower()
    body = (p.get("selftext") or "").lower()

    bonus = [
        "slippage","execution","fill","latency","overfit","backtest","walk-forward",
        "ibkr","live","paper","risk","drawdown","fees","spread","market impact",
        "position sizing","leverage","funding","rebalance","market making",
    ]
    if any(w in title or w in body for w in bonus):
        score += 12.0

    if len((p.get("selftext") or "").strip()) < 40:
        score -= 3.0

    return score
def build_candidates(seen_ids: set) -> List[Dict[str, Any]]:
    candidates: List[Dict[str, Any]] = []

    for sub in SUBREDDITS:
        posts = fetch_sub_posts(sub, CANDIDATES_PER_SUB)

        for p in posts:
            pid = p.get("id")
            if not pid or pid in seen_ids:
                continue

            # -----------------------
            # BASE TEXT BODY
            # -----------------------
            body = (p.get("selftext") or "")[:1400]

            img_url = p.get("url") or ""

            # -----------------------
            # VISION ENRICHMENT
            # -----------------------
            if img_url and is_image_url(img_url):
                img_path = download_temp_image(img_url)

                if img_path:
                    try:
                        vision_text = ollama_vision_generate(
                            prompt=(
                                "This is a trading-related image or chart. "
                                "Describe clearly what is happening, any trends, "
                                "positions, PnL, indicators, or important takeaways."
                            ),
                            image_path=img_path
                        )

                        if vision_text:
                            body = (
                                f"{body}\n\n"
                                f"[IMAGE ANALYSIS]\n"
                                f"{vision_text}"
                            )

                    except Exception:
                        pass

                    try:
                        os.remove(img_path)
                    except Exception:
                        pass

            # -----------------------
            # BUILD CANDIDATE OBJECT
            # -----------------------
            candidates.append({
                "id": pid,
                "sub": sub,
                "title": (p.get("title") or "")[:240],

                # ðŸ‘‰ THIS IS WHERE BODY GOES (ENRICHED OR NOT)
                "body": body,

                "score": int(p.get("score", 0)),
                "num_comments": int(p.get("num_comments", 0)),
                "heur": candidate_score(p),
            })

    # rank by heuristic (same as before)
    candidates.sort(key=lambda x: x["heur"], reverse=True)

    return candidates[:30]

# =======================
# Persistence (SQLite queue)
# =======================
def vision_folder_worker(stop_event, conn, mem, folder="vision_inbox"):
    os.makedirs(folder, exist_ok=True)

    seen = set()

    while not stop_event.is_set():
        for fn in os.listdir(folder):
            if fn in seen:
                continue
            if not fn.lower().endswith((".png",".jpg",".jpeg")):
                continue

            path = os.path.join(folder, fn)

            try:
                vision_text = ollama_vision_generate(
                    prompt="Analyze this trading screenshot and summarize what is happening.",
                    image_path=path
                )

                seg = {
                    "id": sha1(fn + str(now_ts())),
                    "post_id": fn,
                    "subreddit": "vision",
                    "source": "vision",
                    "event_type": "screenshot",
                    "title": "Vision snapshot analysis",
                    "body": vision_text,
                    "comments": [],
                    "priority": 88.0,
                }

                db_enqueue_segment(conn, seg)

                seen.add(fn)

            except Exception:
                pass

        time.sleep(5)
def ollama_image_generate(prompt, model="llava:latest"):
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    r = requests.post(OLLAMA_URL, json=payload, timeout=120)
    r.raise_for_status()
    return r.json()

def db_connect() -> sqlite3.Connection:
    conn = sqlite3.connect(DB_PATH, check_same_thread=False)
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("""
    CREATE TABLE IF NOT EXISTS segments (
        id TEXT PRIMARY KEY,
        created_ts INTEGER,
        priority REAL,
        status TEXT,           -- queued | claimed | done | dropped
        post_id TEXT,
        subreddit TEXT,
        source TEXT,
        event_type TEXT,
        title TEXT,
        body TEXT,
        comments_json TEXT,
        angle TEXT,
        why TEXT,
        key_points_json TEXT,
        host_hint TEXT
    );
    """)

    conn.execute("""
    CREATE TABLE IF NOT EXISTS seen_posts (
        post_id TEXT PRIMARY KEY,
        first_seen_ts INTEGER
    );
    """)
    conn.commit()
    return conn
def migrate_segments_table(conn: sqlite3.Connection):
    cur = conn.execute("PRAGMA table_info(segments);")
    cols = [r[1] for r in cur.fetchall()]

    if "source" not in cols:
        conn.execute("ALTER TABLE segments ADD COLUMN source TEXT;")

    if "event_type" not in cols:
        conn.execute("ALTER TABLE segments ADD COLUMN event_type TEXT;")

    conn.commit()

def db_queue_depth(conn: sqlite3.Connection) -> int:
    cur = conn.execute("SELECT COUNT(*) FROM segments WHERE status='queued';")
    return int(cur.fetchone()[0])

def db_seen_set(conn: sqlite3.Connection) -> set:
    cur = conn.execute("SELECT post_id FROM seen_posts;")
    return set(r[0] for r in cur.fetchall())

def db_mark_seen(conn: sqlite3.Connection, post_ids: List[str]) -> None:
    ts = now_ts()
    for pid in post_ids:
        conn.execute("INSERT OR IGNORE INTO seen_posts(post_id, first_seen_ts) VALUES (?, ?);", (pid, ts))
    conn.commit()

def db_enqueue_segment(conn: sqlite3.Connection, seg: Dict[str, Any]) -> None:
    """
    Enqueue a segment with stream-aware origin columns.

    Required keys:
      id, post_id, subreddit, title, body
    Optional keys:
      source, event_type, comments, angle, why, key_points, host_hint, priority
    """
    source = seg.get("source", None)
    event_type = seg.get("event_type", None)

    # Back-compat: if caller didn't set these, infer something sane.
    if not source:
        # If this looks like an event segment, prefer its subreddit-as-source pattern
        source = seg.get("subreddit", "reddit")
    if not event_type:
        event_type = "post"

    conn.execute("""
    INSERT OR IGNORE INTO segments (
        id, created_ts, priority, status,
        post_id, subreddit, source, event_type,
        title, body,
        comments_json, angle, why, key_points_json, host_hint
    ) VALUES (?, ?, ?, 'queued', ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);
    """, (
        seg["id"],
        now_ts(),
        float(seg.get("priority", 50.0)),
        seg["post_id"],
        seg["subreddit"],
        source,
        event_type,
        seg["title"],
        seg["body"],
        json.dumps(seg.get("comments", []), ensure_ascii=False),
        seg.get("angle", ""),
        seg.get("why", ""),
        json.dumps(seg.get("key_points", []), ensure_ascii=False),
        seg.get("host_hint", ""),
    ))
    conn.commit()


def db_pop_next_segment(conn: sqlite3.Connection) -> Optional[Dict[str, Any]]:
    cur = conn.execute("""
    SELECT id, post_id, subreddit, source, event_type,
           title, body, comments_json, angle, why, key_points_json, host_hint
    FROM segments
    WHERE status='queued'
    ORDER BY priority DESC, created_ts ASC
    LIMIT 1;
    """)
    row = cur.fetchone()
    if not row:
        return None

    seg_id = row[0]
    conn.execute("UPDATE segments SET status='claimed' WHERE id=?;", (seg_id,))
    conn.commit()

    return {
        "id": row[0],
        "post_id": row[1],
        "subreddit": row[2],
        "source": row[3],
        "event_type": row[4],
        "title": row[5],
        "body": row[6],
        "comments": json.loads(row[7] or "[]"),
        "angle": row[8],
        "why": row[9],
        "key_points": json.loads(row[10] or "[]"),
        "host_hint": row[11],
    }


def db_mark_done(conn: sqlite3.Connection, seg_id: str) -> None:
    conn.execute("UPDATE segments SET status='done' WHERE id=?;", (seg_id,))
    conn.commit()

# =======================
# Prompts
# =======================

def context_system(mem: Dict[str, Any]) -> str:
    themes = mem.get("themes", [])[-12:]
    callbacks = mem.get("callbacks", [])[-10:]

    return f"""
You are the background PRODUCER for {SHOW_NAME}.

You are NOT a breaking news bot.
You curate the ongoing narrative of the station over hours and days.

Your job is to shape:

â€¢ what topics get airtime
â€¢ how deep each topic goes
â€¢ how segments contrast or build on each other
â€¢ which themes should be emphasized today

You think like a real radio producer/editor.

---

High-level goals:

- Favor substance: execution, validation, risk, infra, post-mortems
- Avoid repetitive beginner content unless comments add new insight
- Maintain variety while leaning into trending themes
- Occasionally build mini story arcs across multiple segments
Tag rules:

â€¢ Tags must be generic and reusable
â€¢ snake_case only
â€¢ 1â€“4 words max
â€¢ No post-specific names
â€¢ Prefer core concepts

Good:
execution
slippage
risk_management
overfitting
strategy_design
infra
market_regime

Bad:
ibkr_issue_today
johns_strategy
weird_fill_problem

---

For each chosen segment you must decide:

1) The ANGLE (how the host should frame it)
2) WHY it matters right now
3) KEY POINTS to emphasize
4) PRIORITY (0â€“100)
5) DEPTH:
   - "quick"  â†’ short hit
   - "deep"   â†’ allow longer discussion / rambling
6) CONTRAST STYLE (optional):
   - "debate" â†’ strongly opposing viewpoints
   - "analysis" â†’ technical breakdown
   - "trend" â†’ connect to larger theme
   - "story" â†’ real-world experience focus

---

Output STRICT JSON ONLY (no extra text):

{{
  "segments": [
    {{
      "post_id": "abc123",
      "angle": "...",
      "why": "...",
      "key_points": ["...", "...", "..."],

      "tags": [
        "execution",
        "slippage",
        "overfitting",
        "risk_management",
        "infra",
        "live_vs_backtest",
        "position_sizing",
        "latency",
        "market_regime",
        "strategy_design"
      ],

      "priority": 0-100,
      "depth": "quick | deep",
      "contrast_style": "...",
      "host_hint": "..."
    }}
  ]
}}


---

Narrative awareness:

Current recurring themes:
{themes}

Recent callbacks:
{callbacks}

Use these to:

â€¢ bias topic selection
â€¢ build continuity
â€¢ intentionally contrast ideas
â€¢ deepen important trends

---

Think in terms of shaping a SHOW, not just selecting posts.
""".strip()


def context_prompt(candidates: List[Dict[str, Any]]) -> str:
    lines = []
    for i, c in enumerate(candidates, 1):
        lines.append(
            f"{i}) [{c['id']}] r/{c['sub']} score {c['score']} comments {c['num_comments']} heur {c['heur']:.1f}\n"
            f"Title: {c['title']}\n"
            f"Body: {c['body'][:500]}\n"
        )
    return "Choose segments for the upcoming show queue from these candidates:\n\n" + "\n".join(lines)

def host_system(mem: Dict[str, Any]) -> str:
    themes = mem.get("themes", [])[-12:]
    callbacks = mem.get("callbacks", [])[-10:]
    return f"""
You are {HOST_NAME}, host of {SHOW_NAME}.
Tone: chill, smart, not hypey. Spoken words only. No URLs.
You NEVER announce what you are about to do.

You never say phrases like:
"sure"
"hereâ€™s a script"
"hereâ€™s a breakdown"
"letâ€™s talk about"
"I will now"

You simply speak naturally like a radio host already mid-show.

You never say: "as an AI". No bullet lists in final delivery.
Avoid financial advice.

Continuity:
Themes: {themes}
Callbacks: {callbacks}
""".strip()

def host_packet_system(mem: Dict[str, Any]) -> str:
    themes = mem.get("themes", [])[-12:]
    callbacks = mem.get("callbacks", [])[-10:]

    strategy_ref = get_strategy_reference()
    strategy_block = ""
    if strategy_ref:
        strategy_block = f"""

---
TRADING SYSTEM CONTEXT (for framing only):

{strategy_ref}

Use this to interpret behavior naturally.
Do NOT quote code.
"""

    coach_ref = get_coach_reference()
    coach_block = ""
    if coach_ref:
        coach_block = f"""

---
CAREER ARC:

{coach_ref}

Keep long-horizon consistency.
"""

    coach_active = coach_prompt_context(mem)
    coach_active_block = ""
    if coach_active:
        coach_active_block = f"""

---
COACH CHECK-IN (relevant now):

{coach_active}

Paraphrase naturally.
"""

    return f"""
You are {HOST_NAME}, host of {SHOW_NAME}.

You are LIVE on air.

Natural radio flow.
No scripts.
No bullet points.
No announcements of actions.
No stock phrases.
No disclaimers.

---

Segment sources:

REDDIT â†’ break down ideas + comments  
CHART â†’ react to live changes  
PORTFOLIO â†’ interpret risk + exposure  
COACH â†’ reinforce long-term discipline  

---

Side voices represent different thinking styles.

They must NOT repeat ideas, structure, or phrasing.

Each voice must introduce a DISTINCT dimension.

Roles rotate angles.

SKEPTIC may question:
- data bias
- regime dependency
- tail risk
- scaling effects
- assumption fragility
- correlation shifts

ENGINEER may focus on:
- execution mechanics
- sizing math
- slippage behavior
- infra limits
- risk controls
- system stability

MACRO may connect to:
- volatility cycles
- liquidity regimes
- risk-on vs risk-off
- policy shifts
- structural changes

OPTIMIST may emphasize:
- robustness
- adaptation
- compounding
- diversification
- learning effects

Do NOT reuse the same type of angle in one segment.

---

IMPORTANT:

If reacting to comments, reference them by index.

---

OUTPUT STRICT JSON ONLY:

{{
  "host_intro": "...",
  "summary": "...",
  "perspectives": [
    {{
      "sentiment": "bullish | bearish | cautious | practical | contrarian | curious",
      "voice": "optimist | skeptic | engineer | macro",
      "line": "...",
      "comment_index": 0
    }}
  ],
  "host_takeaway": "...",
  "callback": "optional short reference",
  "commentary_cues": [
    {{ "type": "highlight_comment", "index": 0 }}
  ]
}}

---

Continuity:

Themes: {themes}
Callbacks: {callbacks}

{strategy_block}
{coach_block}
{coach_active_block}

Sound like a real flowing show.
""".strip()
def remember_voice_angles(mem: Dict[str, Any], packet: Dict[str, Any]) -> None:
    recent = mem.setdefault("recent_voice_usage", [])

    for p in packet.get("perspectives", []):
        recent.append({
            "voice": p.get("voice"),
            "sentiment": p.get("sentiment")
        })

    mem["recent_voice_usage"] = recent[-40:]
    save_memory(mem)


def host_prompt_for_segment(seg: Dict[str, Any]) -> str:
    source = seg.get("source", "reddit")
    event_type = seg.get("event_type", "post")

    # Keep comments short in prompt; model should pick indices
    comments = seg.get("comments", []) or []
    comment_lines = []
    for i, c in enumerate(comments[:TOP_COMMENTS]):
        comment_lines.append(f"[{i}] {c}")

    comments_block = "\n".join(comment_lines) if comment_lines else "(no comments available)"

    return f"""
You are going on-air for ONE segment.

SEGMENT META:
source: {source}
event_type: {event_type}
subreddit: {seg.get('subreddit','')}
post_id: {seg.get('post_id','')}

TITLE:
{seg.get('title','')}

PRODUCER NOTES:
angle: {seg.get('angle','')}
why: {seg.get('why','')}
key_points: {seg.get('key_points', [])}
opening_hint: {seg.get('host_hint','')}

PRIMARY MATERIAL (trimmed):
{(seg.get('body','') or '')[:1200]}

COMMENTS (reference by index; paraphrase):
{comments_block}

Remember:
- If you react to a comment, include comment_index in that perspective.
- Keep spoken lines short and conversational.
""".strip()

# =======================
# Producer thread (context engine)
# =======================

def parse_json_strict(text: str) -> Dict[str, Any]:
    text = text.strip()
    m = re.search(r"\{.*\}", text, flags=re.S)
    if m:
        text = m.group(0)
    return json.loads(text)

def producer_loop(stop_event: threading.Event, conn: sqlite3.Connection, mem: Dict[str, Any]) -> None:
    """
    Background context engine:
    - builds candidate pool
    - asks big model to plan segments
    - enqueues quickly (NO comment blocking)
    - bumps tag heat + narrative memory
    """

    while not stop_event.is_set():
        try:
            depth = db_queue_depth(conn)

            # If queue healthy, chill
            if depth >= QUEUE_TARGET_DEPTH:
                time.sleep(PRODUCER_TICK_SEC)
                continue

            seen = db_seen_set(conn)
            candidates = build_candidates(seen)

            if not candidates:
                time.sleep(PRODUCER_TICK_SEC)
                continue

            sys = context_system(mem)
            prm = context_prompt(candidates)

            raw = ollama(
                prm,
                sys,
                model=CONTEXT_MODEL,
                num_predict=520,
                temperature=0.35
            )

            try:
                plan = parse_json_strict(raw)
            except Exception:
                # Safe fallback if model derps
                top = candidates[:3]
                plan = {
                    "segments": [{
                        "post_id": t["id"],
                        "angle": "Pull out practical lessons and real-world friction.",
                        "why": "Strong engagement and likely production insight.",
                        "key_points": ["execution realities", "assumption failures", "risk controls"],
                        "tags": ["execution", "risk_management", "live_vs_backtest"],
                        "priority": 60 + t["heur"] / 4,
                        "host_hint": "Alrightâ€”this one highlights where theory meets reality."
                    } for t in top],
                    "new_themes": [],
                    "new_callbacks": [],
                }

            segs = plan.get("segments", [])[:4]
            by_id = {c["id"]: c for c in candidates}

            enqueued_post_ids = []

            for s in segs:
                pid = s.get("post_id")
                c = by_id.get(pid)

                if not c:
                    continue

                if db_queue_depth(conn) >= QUEUE_MAX_DEPTH:
                    break

                seg_obj = {
                    "id": sha1(pid + "|" + str(now_ts()) + "|" + str(random.random())),
                    "post_id": pid,

                    # Keep this as the subreddit name for display / filtering
                    "subreddit": c["sub"],

                    # NEW: stream origin
                    "source": "reddit",
                    "event_type": "post",

                    "title": c["title"],
                    "body": c["body"],
                    "comments": [],

                    "angle": s.get("angle", ""),
                    "why": s.get("why", ""),
                    "key_points": s.get("key_points", []),
                    "priority": float(s.get("priority", 55.0)),
                    "host_hint": s.get("host_hint", ""),
                }


                db_enqueue_segment(conn, seg_obj)
                enqueued_post_ids.append(pid)

                # ---- TAG HEAT ----
                raw_tags = s.get("tags", [])
                tags = normalize_tags(raw_tags)
                priority = float(s.get("priority", 55.0))

                if tags:
                    bump_tag_heat(mem, tags, boost=priority * 0.4)

            # ---- Narrative memory ----
            new_themes = [t for t in plan.get("new_themes", []) if isinstance(t, str)]
            new_callbacks = [c for c in plan.get("new_callbacks", []) if isinstance(c, str)]

            if new_themes:
                mem["themes"] = (mem.get("themes", []) + new_themes)[-60:]

            if new_callbacks:
                mem["callbacks"] = (mem.get("callbacks", []) + new_callbacks)[-40:]

            save_memory(mem)

            if enqueued_post_ids:
                db_mark_seen(conn, enqueued_post_ids)

        except Exception as e:
            print("Producer error:", e)

        time.sleep(PRODUCER_TICK_SEC)


# =======================
# NEW: Host packet -> audio bundle pipeline
# =======================


audio_queue: "queue.Queue[List[Tuple[str,str]]]" = queue.Queue(maxsize=AUDIO_MAX_DEPTH)

def packet_to_audio_bundle(packet: Dict[str, Any]) -> List[Tuple[str, str]]:
    bundle: List[Tuple[str, str]] = []

    host_intro = clean(packet.get("host_intro", ""))
    summary = clean(packet.get("summary", ""))
    takeaway = clean(packet.get("host_takeaway", ""))

    if host_intro:
        bundle.append(("host", host_intro))
    if summary:
        bundle.append(("host", summary))

    perspectives = packet.get("perspectives", []) or []

    transitions = [
        ("One angle here.", None),
        ("Another perspective.", None),
        ("A different concern.", None),
        ("From a technical lens.", None),
        ("Looking bigger picture.", None),
    ]

    for p in perspectives[:4]:
        voice = str(p.get("voice", "skeptic")).strip().lower()
        if voice not in VOICE_MAP:
            voice = "skeptic"

        line = clean(str(p.get("line", "")))
        if not line:
            continue

        idx = p.get("comment_index", None)

        t, _ = random.choice(transitions)

        if idx is not None:
            try:
                ui_q.put(("highlight_comment", int(idx)))
            except Exception:
                pass

        bundle.append(("host", t))
        bundle.append((voice, line))

    if takeaway:
        bundle.append(("host", takeaway))

    cb = clean(packet.get("callback", ""))
    if cb:
        bundle.append(("host", f"And that ties back to {cb}."))

    return bundle

def render_segment_audio(seg: Dict[str, Any], mem: Dict[str, Any]) -> List[Tuple[str, str]]:
    sys = host_packet_system(mem)
    prm = host_prompt_for_segment(seg)

    raw = ollama(
        prm,
        sys,
        model=HOST_MODEL,
        num_predict=520,
        temperature=0.75   # higher = less collapse
    )

    try:
        packet = parse_json_strict(raw)

    except Exception:
        # ðŸš« NO TEMPLATE FALLBACK
        # just let host talk naturally if model fails

        txt = ollama(
            prompt=(
                f"React naturally on air to this segment in a short flowing way:\n\n"
                f"{seg.get('title','')}\n\n{seg.get('body','')[:800]}"
            ),
            system=host_system(mem),
            model=HOST_MODEL,
            num_predict=220,
            temperature=0.75
        )

        return [("host", clean(txt))]

    # -----------------------------
    # ðŸŽ² ENTROPY: randomize order
    # -----------------------------
    perspectives = packet.get("perspectives", []) or []
    random.shuffle(perspectives)
    packet["perspectives"] = perspectives

    # -----------------------------
    # ðŸ§  light memory (optional)
    # -----------------------------
    try:
        remember_voice_angles(mem, packet)
    except Exception:
        pass

    return packet_to_audio_bundle(packet)


def tts_worker(
    stop_event: threading.Event,
    conn: sqlite3.Connection,
    mem: Dict[str, Any]
) -> None:

    last_source = None
    last_audio_ts = 0

    while not stop_event.is_set():
        try:
            if audio_queue.qsize() >= AUDIO_TARGET_DEPTH:
                time.sleep(0.12)
                continue

            seg = db_pop_next_segment(conn)

            if not seg:
                time.sleep(0.12)
                continue

            source = seg.get("source", "reddit")
            priority = float(seg.get("priority", 50.0))

            now = now_ts()

            # -----------------------
            # ðŸš¦ SOURCE PACING RULE
            # -----------------------

            # Avoid portfolio back-to-back unless high priority
            if (
                source == "portfolio"
                and last_source == "portfolio"
                and priority < 96.0
                and now - last_audio_ts < 180
            ):
                # push it back softly by re-queuing later
                seg["priority"] -= 5
                db_enqueue_segment(conn, seg)
                db_mark_done(conn, seg["id"])
                continue

            try:
                bundle = render_segment_audio(seg, mem)
            except Exception:
                db_mark_done(conn, seg["id"])
                continue

            try:
                audio_queue.put(bundle, timeout=1.0)

                last_source = source
                last_audio_ts = now

                db_mark_done(conn, seg["id"])

            except queue.Full:
                db_mark_done(conn, seg["id"])

        except Exception:
            time.sleep(0.2)

# =======================
# Host loop (plays audio bundles; never blocks on producer/tts)
# =======================

def station_id(mem: Dict[str, Any]) -> None:
    if now_ts() - int(mem.get("last_station_id", 0)) > 240:
        speak(f"Youâ€™re tuned to {SHOW_NAME}.", "host")
        mem["last_station_id"] = now_ts()
        save_memory(mem)

def evergreen_riff(mem: Dict[str, Any]) -> str:
    themes = mem.get("themes", [])[-12:]
    callbacks = mem.get("callbacks", [])[-8:]

    return f"""
You have a short gap on air.

Talk casually and naturally for about {HOST_IDLE_RIFF_SEC} seconds.

You may:
- reflect on a recent theme
- connect two ideas together
- clarify a misconception you've seen lately
- tease something the show has been covering

Use the stationâ€™s recent context naturally.

Recent themes:
{themes}

Recent callbacks:
{callbacks}

Keep it conversational and flowing.
No bullet points.
No generic filler.
""".strip()

def host_loop(stop_event: threading.Event, conn: sqlite3.Connection, mem: Dict[str, Any]) -> None:
    """
    Playback loop:
    - If audio buffer has content: play it.
    - If buffer is low/empty: generate a short riff in host voice (keeps station alive).
    """
    while not stop_event.is_set():
        station_id(mem)

        try:
            bundle = audio_queue.get_nowait()
            play_audio_bundle(bundle)
            time.sleep(HOST_BETWEEN_SEGMENTS_SEC)
            continue
        except queue.Empty:
            pass

        # No audio ready: riff (fast model, host voice). This is the only place we allow blocking.
        txt = ollama(
            prompt=heat_riff_prompt(mem),
            system=host_system(mem),
            model=HOST_MODEL,
            num_predict=420,
            temperature=0.65
        )

        speak(txt, "host")
        time.sleep(HOST_BETWEEN_SEGMENTS_SEC)
def db_update_comments(conn: sqlite3.Connection, post_id: str, comments: List[str]) -> None:
    conn.execute(
        "UPDATE segments SET comments_json=? WHERE post_id=? AND status IN ('queued','claimed');",
        (json.dumps(comments, ensure_ascii=False), post_id)
    )
    conn.commit()

def db_find_missing_comments(conn: sqlite3.Connection, limit: int = 5) -> List[str]:
    cur = conn.execute("""
        SELECT post_id FROM segments
        WHERE status='queued' AND (comments_json IS NULL OR comments_json='[]')
        AND subreddit != 'station'
        ORDER BY created_ts DESC
        LIMIT ?;
    """, (limit,))
    return [r[0] for r in cur.fetchall()]
def comments_backfill_worker(stop_event: threading.Event, conn: sqlite3.Connection) -> None:
    while not stop_event.is_set():
        try:
            pids = db_find_missing_comments(conn, limit=5)
            for pid in pids:
                comments = fetch_comments(pid, TOP_COMMENTS)
                if comments:
                    db_update_comments(conn, pid, comments)
            time.sleep(0.8)
        except Exception:
            time.sleep(1.0)

# =======================
# Entry
# =======================
def main():
    """
    Corrected boot order + thread ownership:

    - Tk UI MUST run on the main thread (ui.root.mainloop()).
    - All workers run in daemon threads.
    - When the UI window closes (or Ctrl+C), we set stop_event so workers can exit cleanly.
    - Removes the broken subtitle_window usage (you only have ONE Tk root: ui.root).
    """

    # --- UI on main thread ---
    ui = StationUI()

    # quick sanity check
    if "..." in VOICE_MAP.get("optimist", ""):
        print("NOTE: VOICE_MAP has placeholder paths ('...'). Update to real .onnx files.")

    # --- station state (shared across workers) ---
    mem = load_memory()
    conn = db_connect()
    migrate_segments_table(conn)

    stop_event = threading.Event()

    # When user closes the window, stop workers then close UI
    def on_close():
        stop_event.set()
        try:
            ui.root.quit()
        except Exception:
            pass
        try:
            ui.root.destroy()
        except Exception:
            pass

    ui.root.protocol("WM_DELETE_WINDOW", on_close)

    # --- Threads (all daemon) ---
    threads: List[threading.Thread] = []

    # DJ
    threads.append(threading.Thread(target=dj_worker, args=(stop_event,), daemon=True))

    # Seed
    enqueue_cold_open(conn, mem)

    # Core station
    threads.append(threading.Thread(target=producer_loop, args=(stop_event, conn, mem), daemon=True))
    threads.append(threading.Thread(target=comments_backfill_worker, args=(stop_event, conn), daemon=True))
    threads.append(threading.Thread(target=tts_worker, args=(stop_event, conn, mem), daemon=True))
    threads.append(threading.Thread(target=host_loop, args=(stop_event, conn, mem), daemon=True))

    # Hyperliquid (optional)
    if HL_USER_ADDRESS:
        threads.append(threading.Thread(target=hyperliquid_worker, args=(stop_event, conn, mem), daemon=True))
    else:
        print("Hyperliquid worker disabled (set HL_USER_ADDRESS env var to enable).")

    # Chart watcher + router + coach
    threads.append(threading.Thread(
        target=chart_watcher_worker,
        args=(stop_event, mem, ["BTCUSDT", "ETHUSDT"], ["1m", "5m", "15m"], 3.0),
        daemon=True
    ))
    threads.append(threading.Thread(target=event_router_worker, args=(stop_event, conn, mem), daemon=True))
    threads.append(threading.Thread(target=coach_worker, args=(stop_event, mem), daemon=True))

    # Start everything
    for t in threads:
        t.start()

    # --- Run UI loop on main thread ---
    try:
        ui.root.mainloop()
    except KeyboardInterrupt:
        pass
    finally:
        stop_event.set()
        # (daemons will die with process; this just gives a brief grace period)
        try:
            time.sleep(0.25)
        except Exception:
            pass


if __name__ == "__main__":
    main()
